{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data & Train Model\n",
    "This notebook allows us to load the entire dataset, split it into a proper train/test split and trains the model using K-Fold cross validation.\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essentials\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import copy\n",
    "import glob\n",
    "import shutil\n",
    "import numpy as np\n",
    "import json\n",
    "import importlib\n",
    "import time as time\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Sequential as Seq, Linear as Lin, Conv2d\n",
    "\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch.optim as optim\n",
    "import timm\n",
    "from timm.models import create_model\n",
    "from timm.data import create_transform\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Images\n",
    "import albumentations\n",
    "import albumentations.pytorch\n",
    "\n",
    "import cv2\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from barbar import Bar\n",
    "\n",
    "import utils\n",
    "importlib.reload(utils)\n",
    "\n",
    "from utils import get_data, select_gpu, get_model, get_class_weigths, get_files\n",
    "from utils import My_data, CustomTransforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting GPU 1 with 22322MB free memory\n",
      "Using device: cuda:1\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(f\"cuda:{select_gpu()}\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.deterministic = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This downloads the dataset to the server and unzips it so we can use it. Check the size of the folder before running the rest of the code. Should be around 4GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = './BreaKHis_v1/histology_slides/breast/**/SOB/**/**/**/*.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening /home/jovyan/txt/train.txt.txt\n",
      "Opening /home/jovyan/txt/test.txt.txt\n"
     ]
    }
   ],
   "source": [
    "# All eight classes\n",
    "classes = [\"A\", \"F\", \"TA\", \"PT\", \"DC\", \"LC\", \"MC\", \"PC\"]\n",
    "benign_classes = classes[:4]\n",
    "malignant_classes = classes[4:]\n",
    "\n",
    "# All four zoom levels\n",
    "zooms = [\"40\", \"100\", \"200\", \"400\"]\n",
    "\n",
    "train_dict, test_dict = get_data(full_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data\n",
    "In this section we do the following:\n",
    "1. Store all image paths in a dictionary that is filtered on class and zoom level\n",
    "2. Create a stratified train/test split (based on class and zoom level) and store the image paths of both sets again in a filtered dictionary\n",
    "3. Copy all images from the raw data source to the structured data folder \"dataset/\"\n",
    "4. Create a 5-fold cross validation split for the train dataset\n",
    "5. Prepare all splits for forward pass of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold Split\n",
    "Explanation of K-Fold: https://isheunesu48.medium.com/cross-validation-using-k-fold-with-scikit-learn-cfc44bf1ce6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Train: 6405\n",
      "Validate: 727\n",
      "\n",
      "Fold 2\n",
      "Train: 6407\n",
      "Validate: 725\n",
      "\n",
      "Fold 3\n",
      "Train: 6409\n",
      "Validate: 723\n",
      "\n",
      "Fold 4\n",
      "Train: 6413\n",
      "Validate: 719\n",
      "\n",
      "Fold 5\n",
      "Train: 6417\n",
      "Validate: 715\n",
      "\n",
      "Fold 6\n",
      "Train: 6421\n",
      "Validate: 711\n",
      "\n",
      "Fold 7\n",
      "Train: 6423\n",
      "Validate: 709\n",
      "\n",
      "Fold 8\n",
      "Train: 6427\n",
      "Validate: 705\n",
      "\n",
      "Fold 9\n",
      "Train: 6432\n",
      "Validate: 700\n",
      "\n",
      "Fold 10\n",
      "Train: 6434\n",
      "Validate: 698\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Determine number of folds\n",
    "n_folds = 10\n",
    "\n",
    "# Store train and validate for every fold\n",
    "folds = {str(i): {\"train\": [], \"validate\": []} for i in range(n_folds)}\n",
    "\n",
    "kf = KFold(n_splits=n_folds)\n",
    "\n",
    "for c in classes:\n",
    "    for z in zooms:\n",
    "\n",
    "        # For every class and zoom, create a 5-fold split\n",
    "        for i, (train_index, validate_index) in enumerate(kf.split(train_dict[c][z])):\n",
    "\n",
    "            # Store paths of all train images in fold i\n",
    "            fold_train_img = [img for i, img in enumerate(train_dict[c][z]) if i in train_index]\n",
    "\n",
    "            # Store paths of all validate images in fold i\n",
    "            fold_validate_img = [img for i, img in enumerate(train_dict[c][z]) if i in validate_index]\n",
    "\n",
    "            # Add paths to fold i\n",
    "            folds[str(i)][\"train\"] += fold_train_img\n",
    "            folds[str(i)][\"validate\"] += fold_validate_img\n",
    "\n",
    "# Shuffle images in each train/validate fold to make sure order of classes is mixed\n",
    "for i in range(n_folds):\n",
    "    random.shuffle(folds[str(i)][\"train\"])\n",
    "    random.shuffle(folds[str(i)][\"validate\"])\n",
    "\n",
    "# Check number of train and validate items per fold\n",
    "for k, v in folds.items():\n",
    "    print(\"Fold\", int(k)+1)\n",
    "    print(\"Train:\", len(v[\"train\"]))\n",
    "    print(\"Validate:\", len(v[\"validate\"]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates in validation set\n",
    "sets = []\n",
    "for i in range(n_folds):\n",
    "    sets.append(set(folds[str(i)][\"validate\"]))\n",
    "\n",
    "for i in sets:\n",
    "    for j in sets:\n",
    "        if i == j:\n",
    "            continue\n",
    "\n",
    "        duplicates = i.intersection(j)\n",
    "        if duplicates:\n",
    "            print(\"Duplicates found:\", len(duplicates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data for Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize transformers\n",
    "transform = CustomTransforms()\n",
    "\n",
    "# Create datasets for each fold\n",
    "train_folds = [My_data(folds[str(i)][\"train\"], transforms=transform.get_transform('train')) for i in range(n_folds)]\n",
    "validate_folds = [My_data(folds[str(i)][\"validate\"], transforms=transform.get_transform('valid')) for i in range(n_folds)]\n",
    "\n",
    "# Create data loaders for each fold\n",
    "train_dataloaders = [DataLoader(dataset=train_folds[i], batch_size=4,shuffle=True,num_workers=2,\n",
    "                                              pin_memory=True,prefetch_factor=2) for i in range(n_folds)]\n",
    "\n",
    "validate_dataloaders = [DataLoader(dataset=validate_folds[i], batch_size=4,shuffle=True,num_workers=2,\n",
    "                                              pin_memory=True,prefetch_factor=2) for i in range(n_folds)]\n",
    "\n",
    "test_dataloader = DataLoader(My_data(get_files(\"./dataset/test/original/**/**/*.png\"), transforms=transform.get_transform('test')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0, class_weights=None):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        probs = torch.sigmoid(logits)\n",
    "        ce_loss = nn.BCELoss()(probs, labels)\n",
    "        # print(type(probs), probs, self.gamma)\n",
    "        weight = (1 - probs).pow(self.gamma)\n",
    "        loss = ce_loss  # Initialize loss with cross-entropy loss\n",
    "        if self.class_weights is not None:\n",
    "            weight = weight * self.class_weights\n",
    "            loss = loss * weight\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "This section performs the actual training of the model. We first determine methods fit and validate that will be called during the training. Afterwards, we define the loop that optimizes the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, dataloader, optimizer,scheduler, criterion):\n",
    "    #print('Training')\n",
    "    model.train()\n",
    "    train_running_loss = 0.0\n",
    "    train_running_correct = 0\n",
    "    accum_iter = 4\n",
    "\n",
    "    for i, (inputs, labels) in enumerate(Bar(dataloader)):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.float().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        #model.zero_grad(set_to_none=True)\n",
    "        # Forward pass - compute outputs on input data using the model\n",
    "        outputs = model(inputs)\n",
    "        thresholds = [0.5, 0.5, 0.5,0.5,0.5,0.5,0.5,0.5]\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        train_running_loss += loss.item()* inputs.size(0)\n",
    "        # _ , preds = torch.max(outputs.data, 1)\n",
    "        # Apply sigmoid activation to obtain probabilities\n",
    "        #preds = (outputs > 0.5).float()\n",
    "        probs = torch.sigmoid(outputs)\n",
    "        preds = torch.zeros_like(probs)\n",
    "\n",
    "        # Set predicted labels based on the threshold\n",
    "        for i, threshold in enumerate(thresholds):\n",
    "            preds[:, i] = (probs[:, i] >= threshold).float()\n",
    "        train_running_correct += (preds == labels).all(dim=1).float().sum()\n",
    "        # Backpropagate the gradients\n",
    "        loss /= accum_iter\n",
    "        loss.backward()\n",
    "\n",
    "        if ((i + 1) % accum_iter == 0) :\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    train_loss = train_running_loss/len(dataloader.dataset)\n",
    "    train_accuracy = 100. * train_running_correct/len(dataloader.dataset)\n",
    "    return train_loss, train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader, optimizer, criterion):\n",
    "    #print('Validating')\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    val_running_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.float()\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            thresholds = [0.5, 0.5, 0.5,0.5,0.5,0.5,0.5,0.5]\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_running_loss += loss.item()*inputs.size(0)\n",
    "            #_, preds = torch.max(outputs.data, 1)\n",
    "            #preds = (outputs > 0.5).float()\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            preds = torch.zeros_like(probs)\n",
    "            # Set predicted labels based on the threshold\n",
    "            for i, threshold in enumerate(thresholds):\n",
    "                preds[:, i] = (probs[:, i] >= threshold).float()\n",
    "            val_running_correct += (preds == labels).all(dim=1).float().sum()\n",
    "\n",
    "    val_loss = val_running_loss/len(dataloader.dataset)\n",
    "    val_accuracy = 100. * val_running_correct/len(dataloader.dataset)\n",
    "    return val_loss, val_accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_weights(m):\n",
    "  '''\n",
    "    Try resetting model weights to avoid\n",
    "    weight leakage.\n",
    "  '''\n",
    "  for layer in m.children():\n",
    "   if hasattr(layer, 'reset_parameters'):\n",
    "    # print(f'Reset trainable parameters of layer = {layer}')\n",
    "    layer.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------\n",
      "FOLD 0\n",
      "-----------------------------------------------\n",
      "Reset trainable parameters of layer = Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
      "Reset trainable parameters of layer = LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=2, out_features=512, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=512, out_features=3, bias=False)\n",
      "Reset trainable parameters of layer = Linear(in_features=96, out_features=288, bias=False)\n",
      "Reset trainable parameters of layer = Linear(in_features=96, out_features=96, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=96, out_features=384, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=384, out_features=96, bias=True)\n",
      "Reset trainable parameters of layer = LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "Reset trainable parameters of layer = LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=2, out_features=512, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=512, out_features=3, bias=False)\n",
      "Reset trainable parameters of layer = Linear(in_features=96, out_features=288, bias=False)\n",
      "Reset trainable parameters of layer = Linear(in_features=96, out_features=96, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=96, out_features=384, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=384, out_features=96, bias=True)\n",
      "Reset trainable parameters of layer = LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "Reset trainable parameters of layer = LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=384, out_features=192, bias=False)\n",
      "Reset trainable parameters of layer = LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=2, out_features=512, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=512, out_features=6, bias=False)\n",
      "Reset trainable parameters of layer = Linear(in_features=192, out_features=576, bias=False)\n",
      "Reset trainable parameters of layer = Linear(in_features=192, out_features=192, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=192, out_features=768, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=768, out_features=192, bias=True)\n",
      "Reset trainable parameters of layer = LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "Reset trainable parameters of layer = LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=2, out_features=512, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=512, out_features=6, bias=False)\n",
      "Reset trainable parameters of layer = Linear(in_features=192, out_features=576, bias=False)\n",
      "Reset trainable parameters of layer = Linear(in_features=192, out_features=192, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=192, out_features=768, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=768, out_features=192, bias=True)\n",
      "Reset trainable parameters of layer = LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "Reset trainable parameters of layer = LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=768, out_features=384, bias=False)\n",
      "Reset trainable parameters of layer = LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=2, out_features=512, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=512, out_features=12, bias=False)\n",
      "Reset trainable parameters of layer = Linear(in_features=384, out_features=1152, bias=False)\n",
      "Reset trainable parameters of layer = Linear(in_features=384, out_features=384, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=384, out_features=1536, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=1536, out_features=384, bias=True)\n",
      "Reset trainable parameters of layer = LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "Reset trainable parameters of layer = LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=2, out_features=512, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=512, out_features=12, bias=False)\n",
      "Reset trainable parameters of layer = Linear(in_features=384, out_features=1152, bias=False)\n",
      "Reset trainable parameters of layer = Linear(in_features=384, out_features=384, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=384, out_features=1536, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=1536, out_features=384, bias=True)\n",
      "Reset trainable parameters of layer = LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "Reset trainable parameters of layer = LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=2, out_features=512, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=512, out_features=12, bias=False)\n",
      "Reset trainable parameters of layer = Linear(in_features=384, out_features=1152, bias=False)\n",
      "Reset trainable parameters of layer = Linear(in_features=384, out_features=384, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=384, out_features=1536, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=1536, out_features=384, bias=True)\n",
      "Reset trainable parameters of layer = LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "Reset trainable parameters of layer = LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=2, out_features=512, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=512, out_features=12, bias=False)\n",
      "Reset trainable parameters of layer = Linear(in_features=384, out_features=1152, bias=False)\n",
      "Reset trainable parameters of layer = Linear(in_features=384, out_features=384, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=384, out_features=1536, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=1536, out_features=384, bias=True)\n",
      "Reset trainable parameters of layer = LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "Reset trainable parameters of layer = LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=2, out_features=512, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=512, out_features=12, bias=False)\n",
      "Reset trainable parameters of layer = Linear(in_features=384, out_features=1152, bias=False)\n",
      "Reset trainable parameters of layer = Linear(in_features=384, out_features=384, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=384, out_features=1536, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=1536, out_features=384, bias=True)\n",
      "Reset trainable parameters of layer = LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "Reset trainable parameters of layer = LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=2, out_features=512, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=512, out_features=12, bias=False)\n",
      "Reset trainable parameters of layer = Linear(in_features=384, out_features=1152, bias=False)\n",
      "Reset trainable parameters of layer = Linear(in_features=384, out_features=384, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=384, out_features=1536, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=1536, out_features=384, bias=True)\n",
      "Reset trainable parameters of layer = LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "Reset trainable parameters of layer = LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=1536, out_features=768, bias=False)\n",
      "Reset trainable parameters of layer = LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=2, out_features=512, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=512, out_features=24, bias=False)\n",
      "Reset trainable parameters of layer = Linear(in_features=768, out_features=2304, bias=False)\n",
      "Reset trainable parameters of layer = Linear(in_features=768, out_features=768, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=768, out_features=3072, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=3072, out_features=768, bias=True)\n",
      "Reset trainable parameters of layer = LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "Reset trainable parameters of layer = LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=2, out_features=512, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=512, out_features=24, bias=False)\n",
      "Reset trainable parameters of layer = Linear(in_features=768, out_features=2304, bias=False)\n",
      "Reset trainable parameters of layer = Linear(in_features=768, out_features=768, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=768, out_features=3072, bias=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=3072, out_features=768, bias=True)\n",
      "Reset trainable parameters of layer = LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "Reset trainable parameters of layer = LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "Reset trainable parameters of layer = Linear(in_features=768, out_features=8, bias=True)\n",
      "Reset trainable parameters of layer = LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "Epoch-1/1 lr: 0.0001\n",
      " 496/6405: [==>.............................] - ETA 365.6s"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "model_name = \"swin\"\n",
    "criterion = FocalLoss(get_class_weigths(train_dict).to(device))\n",
    "# For fold results\n",
    "results = {}\n",
    "epochs=1\n",
    "\n",
    "for fold in range(n_folds):\n",
    "    print(\"-----------------------------------------------\")\n",
    "    print(f\"FOLD {fold}\")\n",
    "    print(\"-----------------------------------------------\")\n",
    "\n",
    "    history=[]\n",
    "    best_acc = 0.0\n",
    "\n",
    "    train_dataloader = train_dataloaders[fold]\n",
    "    valid_dataloader = validate_dataloaders[fold]\n",
    "\n",
    "    model = get_model(device, model=model_name)\n",
    "    model.apply(reset_weights)\n",
    "\n",
    "    optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.5)\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start = time.time()\n",
    "        print('Epoch-{0}/{1} lr: {2}'.format(epoch+1,epochs ,optimizer.param_groups[0]['lr']))\n",
    "        \n",
    "        # Why is this here???\n",
    "        # if  epoch > 14:\n",
    "        #     for param in model.parameters():\n",
    "        #         param.requires_grad = True\n",
    "        #print(f\"Epoch {epoch+1} of {epochs}\")\n",
    "        train_epoch_loss, train_epoch_accuracy = fit(model,train_dataloader,optimizer,scheduler,criterion)\n",
    "        val_epoch_loss, val_epoch_accuracy = validate(model,valid_dataloader,optimizer,criterion)\n",
    "\n",
    "        epoch_end = time.time()\n",
    "        history.append([epoch+1,train_epoch_loss, train_epoch_accuracy, val_epoch_loss, val_epoch_accuracy,(epoch_end-epoch_start)])\n",
    "        print(f\"Train Loss: {train_epoch_loss:.4f}, Train Acc: {train_epoch_accuracy:.2f},Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_accuracy:.2f},time : {epoch_end-epoch_start:.2f}\")\n",
    "        torch.save({'history':history}, f'models/Master_{model_name}-cv-{fold}_his.pth')\n",
    "        if val_epoch_accuracy > best_acc:\n",
    "            best_acc = val_epoch_accuracy\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "            best_epoch=epoch\n",
    "            torch.save({\n",
    "                'epoch': epoch+1,\n",
    "                'model_state_dict': best_model_wts,\n",
    "                'loss': criterion,\n",
    "                'history':history,\n",
    "                'best_epoch': best_epoch+1,\n",
    "\n",
    "                }, f'models/Master_{model_name}-best-{fold}.pth')\n",
    "            \n",
    "     # Process is complete.\n",
    "    print('Training process has finished. Saving trained model.')\n",
    "    # Print about testing\n",
    "    print('Starting testing')\n",
    "    \n",
    "    # Saving the model\n",
    "    torch.save(model.state_dict(), f'models/Master_{model_name}-final-{fold}.pth')\n",
    "\n",
    "    # Evaluationfor this fold\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "\n",
    "      # Iterate over the test data and generate predictions\n",
    "      for i, data in enumerate(test_dataloader, 0):\n",
    "        \n",
    "        # Get inputs\n",
    "        inputs, targets = data\n",
    "\n",
    "        true_label = torch.argmax(targets.to(device), dim=1)\n",
    "        confs = model(inputs.to(device))\n",
    "        pred_label = torch.argmax(confs, dim=1)\n",
    "    \n",
    "        total += targets.size(0)\n",
    "        correct += (pred_label == true_label).sum().item()\n",
    "\n",
    "      # Print accuracy\n",
    "      print('Accuracy for fold %d: %d %%' % (fold, 100.0 * correct / total))\n",
    "      print('--------------------------------')\n",
    "      results[fold] = 100.0 * (correct / total)\n",
    "    \n",
    "    # Print fold results\n",
    "print(f'K-FOLD CROSS VALIDATION RESULTS FOR {n_folds} FOLDS')\n",
    "print('--------------------------------')\n",
    "sum = 0.0\n",
    "for key, value in results.items():\n",
    "    print(f'Fold {key}: {value} %')\n",
    "    sum += value\n",
    "print(f'Average: {sum/len(results.items())} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

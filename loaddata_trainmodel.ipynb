{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data & Train Model\n",
    "This notebook allows us to load the entire dataset, split it into a proper train/test split and trains the model using K-Fold cross validation.\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essentials\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import copy\n",
    "import glob\n",
    "import shutil\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Sequential as Seq, Linear as Lin, Conv2d\n",
    "\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch.optim as optim\n",
    "import timm\n",
    "from timm.models import create_model\n",
    "from timm.data import create_transform\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Images\n",
    "import albumentations\n",
    "import albumentations.pytorch\n",
    "\n",
    "import cv2\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from barbar import Bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.deterministic = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This downloads the dataset to the server and unzips it so we can use it. Check the size of the folder before running the rest of the code. Should be around 4GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-01-05 15:52:56--  http://www.inf.ufpr.br/vri/databases/BreaKHis_v1.tar.gz\n",
      "Resolving www.inf.ufpr.br (www.inf.ufpr.br)... 200.17.202.113, 2801:82:80ff:8001:216:ccff:feaa:79\n",
      "Connecting to www.inf.ufpr.br (www.inf.ufpr.br)|200.17.202.113|:80... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://www.inf.ufpr.br/vri/databases/BreaKHis_v1.tar.gz [following]\n",
      "--2024-01-05 15:52:57--  https://www.inf.ufpr.br/vri/databases/BreaKHis_v1.tar.gz\n",
      "Connecting to www.inf.ufpr.br (www.inf.ufpr.br)|200.17.202.113|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4273561758 (4.0G) [application/octet-stream]\n",
      "Saving to: ‘BreaKHis_v1.tar.gz’\n",
      "\n",
      "BreaKHis_v1.tar.gz  100%[===================>]   3.98G  17.7MB/s    in 3m 55s  \n",
      "\n",
      "2024-01-05 15:56:54 (17.3 MB/s) - ‘BreaKHis_v1.tar.gz’ saved [4273561758/4273561758]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://www.inf.ufpr.br/vri/databases/BreaKHis_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xf BreaKHis_v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.1G\t./BreaKHis_v1\n"
     ]
    }
   ],
   "source": [
    "!du -sh ./BreaKHis_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "du: cannot access './Master.pth': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!du -sh ./Master.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = './BreaKHis_v1/histology_slides/breast/**/SOB/**/**/**/*.png'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data\n",
    "In this section we do the following:\n",
    "1. Store all image paths in a dictionary that is filtered on class and zoom level\n",
    "2. Create a stratified train/test split (based on class and zoom level) and store the image paths of both sets again in a filtered dictionary\n",
    "3. Copy all images from the raw data source to the structured data folder \"dataset/\"\n",
    "4. Create a 5-fold cross validation split for the train dataset\n",
    "5. Prepare all splits for forward pass of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitted file path: ['DC', '14', '20629', '100', '027.png']\n",
      "A 40 114\n",
      "A 100 113\n",
      "A 200 111\n",
      "A 400 106\n",
      "F 40 253\n",
      "F 100 260\n",
      "F 200 264\n",
      "F 400 237\n",
      "TA 40 149\n",
      "TA 100 150\n",
      "TA 200 140\n",
      "TA 400 130\n",
      "PT 40 109\n",
      "PT 100 121\n",
      "PT 200 108\n",
      "PT 400 115\n",
      "DC 40 864\n",
      "DC 100 903\n",
      "DC 200 896\n",
      "DC 400 788\n",
      "LC 40 156\n",
      "LC 100 170\n",
      "LC 200 163\n",
      "LC 400 137\n",
      "MC 40 205\n",
      "MC 100 222\n",
      "MC 200 196\n",
      "MC 400 169\n",
      "PC 40 145\n",
      "PC 100 142\n",
      "PC 200 135\n",
      "PC 400 138\n"
     ]
    }
   ],
   "source": [
    "# Store all 7909 filepaths in a list\n",
    "all_files = glob.glob(full_dataset)\n",
    "\n",
    "# All eight classes\n",
    "classes = [\"A\", \"F\", \"TA\", \"PT\", \"DC\", \"LC\", \"MC\", \"PC\"]\n",
    "benign_classes = classes[:4]\n",
    "malignant_classes = classes[4:]\n",
    "\n",
    "# All four zoom levels\n",
    "zooms = [\"40\", \"100\", \"200\", \"400\"]\n",
    "\n",
    "# Create dictionary that filters images based on class and zoom level\n",
    "data_dict = {c: {z: [path for path in all_files if path.split(\"_\")[-1].split(\"-\")[0] == c and path.split(\"_\")[-1].split(\"-\")[3] == z] for z in zooms} for c in classes}\n",
    "\n",
    "# For demonstration purposes a splitted file path\n",
    "# We need first element (class) and fourth element (zoom level)\n",
    "print(\"Splitted file path:\", all_files[0].split(\"_\")[-1].split(\"-\"))\n",
    "\n",
    "# Check number of items per class per zoom level\n",
    "for c in data_dict.keys():\n",
    "    for z, v in data_dict[c].items():\n",
    "        print(c, z, len(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty dicts\n",
    "train_dict = {c: {z: [] for z in zooms} for c in classes}\n",
    "test_dict = {c: {z: [] for z in zooms} for c in classes}\n",
    "\n",
    "# Make train/test split\n",
    "train_test_split = 0.9\n",
    "\n",
    "for c in data_dict.keys():\n",
    "    for z, v in data_dict[c].items():\n",
    "        split = math.ceil(train_test_split * len(v))\n",
    "\n",
    "        # Randomly sample from file paths for given class/zoom level\n",
    "        train_images = random.sample(data_dict[c][z], split)\n",
    "\n",
    "        # Take as test data all files that are not in the train data for a given class/zoom level\n",
    "        test_images = np.setdiff1d(data_dict[c][z], train_images)\n",
    "\n",
    "        # Check if error is made\n",
    "        if len(train_images) + len(test_images) != len(v):\n",
    "            print(\"Error in train/test split at {}-{}\".format(c, z))\n",
    "\n",
    "        # Store train and test data in dictionaries\n",
    "        train_dict[c][z] = list(train_images)\n",
    "        test_dict[c][z] = list(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class A, Zoom 40\n",
      "Total: 114, Train: 103, Test: 11\n",
      "Class A, Zoom 100\n",
      "Total: 113, Train: 102, Test: 11\n",
      "Class A, Zoom 200\n",
      "Total: 111, Train: 100, Test: 11\n",
      "Class A, Zoom 400\n",
      "Total: 106, Train: 96, Test: 10\n",
      "Class F, Zoom 40\n",
      "Total: 253, Train: 228, Test: 25\n",
      "Class F, Zoom 100\n",
      "Total: 260, Train: 234, Test: 26\n",
      "Class F, Zoom 200\n",
      "Total: 264, Train: 238, Test: 26\n",
      "Class F, Zoom 400\n",
      "Total: 237, Train: 214, Test: 23\n",
      "Class TA, Zoom 40\n",
      "Total: 149, Train: 135, Test: 14\n",
      "Class TA, Zoom 100\n",
      "Total: 150, Train: 135, Test: 15\n",
      "Class TA, Zoom 200\n",
      "Total: 140, Train: 126, Test: 14\n",
      "Class TA, Zoom 400\n",
      "Total: 130, Train: 117, Test: 13\n",
      "Class PT, Zoom 40\n",
      "Total: 109, Train: 99, Test: 10\n",
      "Class PT, Zoom 100\n",
      "Total: 121, Train: 109, Test: 12\n",
      "Class PT, Zoom 200\n",
      "Total: 108, Train: 98, Test: 10\n",
      "Class PT, Zoom 400\n",
      "Total: 115, Train: 104, Test: 11\n",
      "Class DC, Zoom 40\n",
      "Total: 864, Train: 778, Test: 86\n",
      "Class DC, Zoom 100\n",
      "Total: 903, Train: 813, Test: 90\n",
      "Class DC, Zoom 200\n",
      "Total: 896, Train: 807, Test: 89\n",
      "Class DC, Zoom 400\n",
      "Total: 788, Train: 710, Test: 78\n",
      "Class LC, Zoom 40\n",
      "Total: 156, Train: 141, Test: 15\n",
      "Class LC, Zoom 100\n",
      "Total: 170, Train: 153, Test: 17\n",
      "Class LC, Zoom 200\n",
      "Total: 163, Train: 147, Test: 16\n",
      "Class LC, Zoom 400\n",
      "Total: 137, Train: 124, Test: 13\n",
      "Class MC, Zoom 40\n",
      "Total: 205, Train: 185, Test: 20\n",
      "Class MC, Zoom 100\n",
      "Total: 222, Train: 200, Test: 22\n",
      "Class MC, Zoom 200\n",
      "Total: 196, Train: 177, Test: 19\n",
      "Class MC, Zoom 400\n",
      "Total: 169, Train: 153, Test: 16\n",
      "Class PC, Zoom 40\n",
      "Total: 145, Train: 131, Test: 14\n",
      "Class PC, Zoom 100\n",
      "Total: 142, Train: 128, Test: 14\n",
      "Class PC, Zoom 200\n",
      "Total: 135, Train: 122, Test: 13\n",
      "Class PC, Zoom 400\n",
      "Total: 138, Train: 125, Test: 13\n"
     ]
    }
   ],
   "source": [
    "# Check if splitting went correct\n",
    "\n",
    "# Print number of items in train and test data for class and zoom level\n",
    "for c in classes:\n",
    "    for z in zooms:\n",
    "        print(\"Class {}, Zoom {}\".format(c, z))\n",
    "        print(\"Total: {}, Train: {}, Test: {}\".format(len(data_dict[c][z]), len(train_dict[c][z]), len(test_dict[c][z])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we dump the train and test dictionaries in a json file. This allows us to pass files more easily between projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train text file\n",
    "with open(\"txt/train.txt\", \"w\") as f:\n",
    "    json.dump(train_dict, f)\n",
    "\n",
    "# Create test text file\n",
    "with open(\"txt/test.txt\", \"w\") as f:\n",
    "    f.write(json.dumps(test_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes all images from a train/test dict and copies them to new destination for easy access\n",
    "def copy_images(image_dict, type):\n",
    "\n",
    "    # Get current working directory\n",
    "    current_dir = os.getcwd()\n",
    "\n",
    "    # Check whether the new file structure is already initialised\n",
    "    if not os.path.isdir(os.path.join(current_dir, \"dataset\", \"train\", \"TA\", \"400\")) or not os.path.isdir(os.path.join(current_dir, \"dataset\", \"test\", \"TA\", \"400\")):\n",
    "        for c in classes:\n",
    "            for z in zooms:\n",
    "                os.makedirs(os.path.join(os.getcwd(), \"dataset\", \"test\", c, z))\n",
    "                os.makedirs(os.path.join(os.getcwd(), \"dataset\", \"train\", c, z))\n",
    "\n",
    "    for c in classes:\n",
    "        for z in zooms:\n",
    "\n",
    "            # Destination path\n",
    "            dst = os.path.join(current_dir, \"dataset\", type, c, z)\n",
    "\n",
    "            # Remove all images currently in destination path\n",
    "            for item in os.listdir(dst):\n",
    "                item_path = os.path.join(dst, item)\n",
    "                if os.path.isfile(item_path):\n",
    "                    os.remove(item_path)\n",
    "                elif os.path.isdir(item_path):\n",
    "                    # Use rmtree for directories\n",
    "                    shutil.rmtree(item_path)\n",
    "\n",
    "            # Copy all images from BreaKHis to dataset directory\n",
    "            for image in image_dict[c][z]:\n",
    "                src = os.path.join(image)\n",
    "                dst = os.path.join(current_dir, \"dataset\", type, c, z)\n",
    "\n",
    "                shutil.copy2(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy images to correct directory for easy access\n",
    "copy_images(test_dict, \"test\")\n",
    "copy_images(train_dict, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-12204/100X/SOB_M_LC-14-12204-100-031.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-12204/100X/SOB_M_LC-14-12204-100-032.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-12204/100X/SOB_M_LC-14-12204-100-034.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-12204/100X/SOB_M_LC-14-12204-100-035.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-12204/100X/SOB_M_LC-14-12204-100-036.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-12204/100X/SOB_M_LC-14-12204-100-037.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-12204/100X/SOB_M_LC-14-12204-100-038.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-12204/100X/SOB_M_LC-14-12204-100-039.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-12204/100X/SOB_M_LC-14-12204-100-040.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-12204/100X/SOB_M_LC-14-12204-100-041.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-12204/100X/SOB_M_LC-14-12204-100-042.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-12204/100X/SOB_M_LC-14-12204-100-043.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-12204/100X/SOB_M_LC-14-12204-100-044.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-12204/100X/SOB_M_LC-14-12204-100-045.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-12204/100X/SOB_M_LC-14-12204-100-047.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-12204/100X/SOB_M_LC-14-12204-100-048.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-12204/100X/SOB_M_LC-14-12204-100-049.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-12204/100X/SOB_M_LC-14-12204-100-050.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-12204/100X/SOB_M_LC-14-12204-100-051.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-12204/100X/SOB_M_LC-14-12204-100-052.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-12204/100X/SOB_M_LC-14-12204-100-053.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-12204/100X/SOB_M_LC-14-12204-100-054.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-12204/100X/SOB_M_LC-14-12204-100-055.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-12204/100X/SOB_M_LC-14-12204-100-057.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-12204/100X/SOB_M_LC-14-12204-100-058.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-12204/100X/SOB_M_LC-14-12204-100-059.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-12204/100X/SOB_M_LC-14-12204-100-060.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-13412/100X/SOB_M_LC-14-13412-100-001.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-13412/100X/SOB_M_LC-14-13412-100-002.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-13412/100X/SOB_M_LC-14-13412-100-003.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-13412/100X/SOB_M_LC-14-13412-100-004.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-13412/100X/SOB_M_LC-14-13412-100-005.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-13412/100X/SOB_M_LC-14-13412-100-006.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-13412/100X/SOB_M_LC-14-13412-100-007.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-13412/100X/SOB_M_LC-14-13412-100-008.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-13412/100X/SOB_M_LC-14-13412-100-009.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-13412/100X/SOB_M_LC-14-13412-100-010.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-13412/100X/SOB_M_LC-14-13412-100-011.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-13412/100X/SOB_M_LC-14-13412-100-012.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-13412/100X/SOB_M_LC-14-13412-100-013.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-13412/100X/SOB_M_LC-14-13412-100-014.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-13412/100X/SOB_M_LC-14-13412-100-015.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-13412/100X/SOB_M_LC-14-13412-100-016.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-13412/100X/SOB_M_LC-14-13412-100-017.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-13412/100X/SOB_M_LC-14-13412-100-018.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-13412/100X/SOB_M_LC-14-13412-100-019.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-13412/100X/SOB_M_LC-14-13412-100-020.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-13412/100X/SOB_M_LC-14-13412-100-021.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-13412/100X/SOB_M_LC-14-13412-100-022.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-13412/100X/SOB_M_LC-14-13412-100-024.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-13412/100X/SOB_M_LC-14-13412-100-025.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-13412/100X/SOB_M_LC-14-13412-100-026.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-13412/100X/SOB_M_LC-14-13412-100-027.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-13412/100X/SOB_M_LC-14-13412-100-028.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-13412/100X/SOB_M_LC-14-13412-100-029.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-13412/100X/SOB_M_LC-14-13412-100-030.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-13412/100X/SOB_M_LC-14-13412-100-031.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-13412/100X/SOB_M_LC-14-13412-100-032.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-13412/100X/SOB_M_LC-14-13412-100-033.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570/100X/SOB_M_LC-14-15570-100-002.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570/100X/SOB_M_LC-14-15570-100-003.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570/100X/SOB_M_LC-14-15570-100-005.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570/100X/SOB_M_LC-14-15570-100-006.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570/100X/SOB_M_LC-14-15570-100-007.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570/100X/SOB_M_LC-14-15570-100-008.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570/100X/SOB_M_LC-14-15570-100-009.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570/100X/SOB_M_LC-14-15570-100-010.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570/100X/SOB_M_LC-14-15570-100-011.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570/100X/SOB_M_LC-14-15570-100-012.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570/100X/SOB_M_LC-14-15570-100-014.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570/100X/SOB_M_LC-14-15570-100-015.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570/100X/SOB_M_LC-14-15570-100-016.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570/100X/SOB_M_LC-14-15570-100-017.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570/100X/SOB_M_LC-14-15570-100-018.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570/100X/SOB_M_LC-14-15570-100-020.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570/100X/SOB_M_LC-14-15570-100-021.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570/100X/SOB_M_LC-14-15570-100-024.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570/100X/SOB_M_LC-14-15570-100-025.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570/100X/SOB_M_LC-14-15570-100-026.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570/100X/SOB_M_LC-14-15570-100-027.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570/100X/SOB_M_LC-14-15570-100-028.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570/100X/SOB_M_LC-14-15570-100-030.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570/100X/SOB_M_LC-14-15570-100-031.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570/100X/SOB_M_LC-14-15570-100-032.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570/100X/SOB_M_LC-14-15570-100-033.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570/100X/SOB_M_LC-14-15570-100-034.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570/100X/SOB_M_LC-14-15570-100-035.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570/100X/SOB_M_LC-14-15570-100-036.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570/100X/SOB_M_LC-14-15570-100-037.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570/100X/SOB_M_LC-14-15570-100-038.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570/100X/SOB_M_LC-14-15570-100-039.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570/100X/SOB_M_LC-14-15570-100-040.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570/100X/SOB_M_LC-14-15570-100-041.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570/100X/SOB_M_LC-14-15570-100-042.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570/100X/SOB_M_LC-14-15570-100-043.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570/100X/SOB_M_LC-14-15570-100-044.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570/100X/SOB_M_LC-14-15570-100-045.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570/100X/SOB_M_LC-14-15570-100-046.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570/100X/SOB_M_LC-14-15570-100-047.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570/100X/SOB_M_LC-14-15570-100-048.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570/100X/SOB_M_LC-14-15570-100-049.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570/100X/SOB_M_LC-14-15570-100-050.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570/100X/SOB_M_LC-14-15570-100-051.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570/100X/SOB_M_LC-14-15570-100-052.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570/100X/SOB_M_LC-14-15570-100-053.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570C/100X/SOB_M_LC-14-15570C-100-002.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570C/100X/SOB_M_LC-14-15570C-100-003.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570C/100X/SOB_M_LC-14-15570C-100-004.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570C/100X/SOB_M_LC-14-15570C-100-005.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570C/100X/SOB_M_LC-14-15570C-100-006.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570C/100X/SOB_M_LC-14-15570C-100-007.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570C/100X/SOB_M_LC-14-15570C-100-008.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570C/100X/SOB_M_LC-14-15570C-100-009.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570C/100X/SOB_M_LC-14-15570C-100-010.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570C/100X/SOB_M_LC-14-15570C-100-011.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570C/100X/SOB_M_LC-14-15570C-100-012.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570C/100X/SOB_M_LC-14-15570C-100-013.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570C/100X/SOB_M_LC-14-15570C-100-014.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570C/100X/SOB_M_LC-14-15570C-100-015.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570C/100X/SOB_M_LC-14-15570C-100-016.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570C/100X/SOB_M_LC-14-15570C-100-017.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570C/100X/SOB_M_LC-14-15570C-100-018.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570C/100X/SOB_M_LC-14-15570C-100-019.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570C/100X/SOB_M_LC-14-15570C-100-020.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570C/100X/SOB_M_LC-14-15570C-100-021.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570C/100X/SOB_M_LC-14-15570C-100-022.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570C/100X/SOB_M_LC-14-15570C-100-023.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570C/100X/SOB_M_LC-14-15570C-100-025.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570C/100X/SOB_M_LC-14-15570C-100-026.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570C/100X/SOB_M_LC-14-15570C-100-027.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570C/100X/SOB_M_LC-14-15570C-100-028.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570C/100X/SOB_M_LC-14-15570C-100-029.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570C/100X/SOB_M_LC-14-15570C-100-031.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570C/100X/SOB_M_LC-14-15570C-100-032.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570C/100X/SOB_M_LC-14-15570C-100-033.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570C/100X/SOB_M_LC-14-15570C-100-034.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-15570C/100X/SOB_M_LC-14-15570C-100-035.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-16196/100X/SOB_M_LC-14-16196-100-001.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-16196/100X/SOB_M_LC-14-16196-100-002.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-16196/100X/SOB_M_LC-14-16196-100-003.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-16196/100X/SOB_M_LC-14-16196-100-004.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-16196/100X/SOB_M_LC-14-16196-100-005.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-16196/100X/SOB_M_LC-14-16196-100-006.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-16196/100X/SOB_M_LC-14-16196-100-007.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-16196/100X/SOB_M_LC-14-16196-100-008.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-16196/100X/SOB_M_LC-14-16196-100-009.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-16196/100X/SOB_M_LC-14-16196-100-010.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-16196/100X/SOB_M_LC-14-16196-100-011.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-16196/100X/SOB_M_LC-14-16196-100-012.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-16196/100X/SOB_M_LC-14-16196-100-014.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-16196/100X/SOB_M_LC-14-16196-100-015.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-16196/100X/SOB_M_LC-14-16196-100-016.png\n",
      "./BreaKHis_v1/histology_slides/breast/malignant/SOB/lobular_carcinoma/SOB_M_LC_14-16196/100X/SOB_M_LC-14-16196-100-017.png\n"
     ]
    }
   ],
   "source": [
    "# Check if copying went correct\n",
    "\n",
    "# Print all images of class LC and zoom 100 in train dict\n",
    "# Compare this with images in dataset/train/LC/100\n",
    "for name in sorted(train_dict[\"LC\"][\"100\"]):\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold Split\n",
    "Explanation of K-Fold: https://isheunesu48.medium.com/cross-validation-using-k-fold-with-scikit-learn-cfc44bf1ce6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Train: 5694\n",
      "Validate: 1438\n",
      "\n",
      "Fold 2\n",
      "Train: 5698\n",
      "Validate: 1434\n",
      "\n",
      "Fold 3\n",
      "Train: 5704\n",
      "Validate: 1428\n",
      "\n",
      "Fold 4\n",
      "Train: 5713\n",
      "Validate: 1419\n",
      "\n",
      "Fold 5\n",
      "Train: 5719\n",
      "Validate: 1413\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Determine number of folds\n",
    "n_folds = 5\n",
    "\n",
    "# Store train and validate for every fold\n",
    "folds = {str(i): {\"train\": [], \"validate\": []} for i in range(n_folds)}\n",
    "\n",
    "kf = KFold(n_splits=n_folds)\n",
    "\n",
    "for c in classes:\n",
    "    for z in zooms:\n",
    "\n",
    "        # For every class and zoom, create a 5-fold split\n",
    "        for i, (train_index, validate_index) in enumerate(kf.split(train_dict[c][z])):\n",
    "\n",
    "            # Store paths of all train images in fold i\n",
    "            fold_train_img = [img for i, img in enumerate(train_dict[c][z]) if i in train_index]\n",
    "\n",
    "            # Store paths of all validate images in fold i\n",
    "            fold_validate_img = [img for i, img in enumerate(train_dict[c][z]) if i in validate_index]\n",
    "\n",
    "            # Add paths to fold i\n",
    "            folds[str(i)][\"train\"] += fold_train_img\n",
    "            folds[str(i)][\"validate\"] += fold_validate_img\n",
    "\n",
    "# Shuffle images in each train/validate fold to make sure order of classes is mixed\n",
    "for i in range(n_folds):\n",
    "    random.shuffle(folds[str(i)][\"train\"])\n",
    "    random.shuffle(folds[str(i)][\"validate\"])\n",
    "\n",
    "# Check number of train and validate items per fold\n",
    "for k, v in folds.items():\n",
    "    print(\"Fold\", int(k)+1)\n",
    "    print(\"Train:\", len(v[\"train\"]))\n",
    "    print(\"Validate:\", len(v[\"validate\"]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates in validation set\n",
    "sets = []\n",
    "for i in range(n_folds):\n",
    "    sets.append(set(folds[str(i)][\"validate\"]))\n",
    "\n",
    "for i in sets:\n",
    "    for j in sets:\n",
    "        if i == j:\n",
    "            continue\n",
    "\n",
    "        duplicates = i.intersection(j)\n",
    "        if duplicates:\n",
    "            print(\"Duplicates found:\", len(duplicates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data for Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_data(Dataset):\n",
    "    def __init__(self, data, transforms=None):\n",
    "        self.image_list = data\n",
    "        self.data_len = len(self.image_list)\n",
    "        self.transforms = transforms\n",
    "        self.eicls = [\"A\", \"F\", \"TA\", \"PT\", \"DC\", \"LC\", \"MC\", \"PC\"]\n",
    "\n",
    "    def __getitem__(self, index):       \n",
    "        current_image_path = self.image_list[index]\n",
    "        im_as_im = cv2.imread(current_image_path)\n",
    "        if im_as_im is None:\n",
    "            raise ValueError(f\"Image not found or is corrupted: {current_image_path}\")\n",
    "        im_as_im = cv2.cvtColor(im_as_im, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Perform label encoding for multi-label classification\n",
    "        parts = current_image_path.split('_')[-1].split('-')\n",
    "        if parts[2]==\"13412\":\n",
    "            labels =[0,0,0,0,1,1,0,0]\n",
    "        else:\n",
    "            labels = [int(label == parts[0]) for label in self.eicls]\n",
    "        labels = torch.tensor(labels)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            augmented = self.transforms(image=im_as_im)\n",
    "            im_as_im = augmented['image']\n",
    "\n",
    "        return (im_as_im, labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = {\n",
    "    'train': albumentations.Compose([\n",
    "    albumentations.Resize(256, 256),\n",
    "    albumentations.OneOf([\n",
    "                          albumentations.HorizontalFlip(),\n",
    "                          albumentations.Rotate(limit=45),\n",
    "                          albumentations.VerticalFlip(),\n",
    "                          albumentations.GaussianBlur(),\n",
    "                          albumentations.NoOp()\n",
    "    ], p=1),\n",
    "    albumentations.Normalize(mean=[0, 0, 0], std=[255, 255, 255], max_pixel_value=1.0),\n",
    "    albumentations.pytorch.transforms.ToTensorV2()]),\n",
    "\n",
    "    'valid': albumentations.Compose([\n",
    "    albumentations.Resize(256, 256),\n",
    "    albumentations.Normalize(mean=[0, 0, 0], std=[255, 255, 255], max_pixel_value=1.0), \n",
    "    albumentations.pytorch.transforms.ToTensorV2()]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets for each fold\n",
    "train_folds = [My_data(folds[str(i)][\"train\"], transforms=transform['train']) for i in range(n_folds)]\n",
    "validate_folds = [My_data(folds[str(i)][\"validate\"], transforms=transform['valid']) for i in range(n_folds)]\n",
    "\n",
    "# Create data loaders for each fold\n",
    "train_dataloaders = [DataLoader(dataset=train_folds[i], batch_size=16,shuffle=True,num_workers=2,\n",
    "                                              pin_memory=True,prefetch_factor=2) for i in range(n_folds)]\n",
    "\n",
    "validate_dataloaders = [DataLoader(dataset=validate_folds[i], batch_size=16,shuffle=True,num_workers=2,\n",
    "                                              pin_memory=True,prefetch_factor=2) for i in range(n_folds)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.2232, 0.9754, 1.7378, 2.1744, 0.2868, 1.5779, 1.2469, 1.7619])\n",
      "7132\n"
     ]
    }
   ],
   "source": [
    "# Number of samples in each class\n",
    "# Assumption is made that in each fold, these are the same (since we do stratified split)\n",
    "class_samples = [sum([len(v) for v in z.values()]) for z in train_dict.values()]\n",
    "\n",
    "# Hardcoded from paper\n",
    "# class_samples = [367, 803, 456, 370, 2763, 492, 629, 449]  # Number of samples in each class for training\n",
    "\n",
    "total_samples = sum(class_samples)\n",
    "samples = total_samples/len(class_samples)\n",
    "class_weights = [samples / (s + 1e-8) for s in class_samples]\n",
    "class_weights = torch.tensor(class_weights)\n",
    "print(class_weights)\n",
    "\n",
    "print(sum(class_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=768, out_features=8, bias=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SwinTransformerV2(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (layers): Sequential(\n",
       "    (0): SwinTransformerV2Stage(\n",
       "      (downsample): Identity()\n",
       "      (blocks): ModuleList(\n",
       "        (0): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=3, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=96, out_features=288, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): Identity()\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): Identity()\n",
       "        )\n",
       "        (1): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=3, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=96, out_features=288, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.018)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.018)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): SwinTransformerV2Stage(\n",
       "      (downsample): PatchMerging(\n",
       "        (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
       "        (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (blocks): ModuleList(\n",
       "        (0): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=6, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.036)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.036)\n",
       "        )\n",
       "        (1): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=6, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=192, out_features=576, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.055)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.055)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): SwinTransformerV2Stage(\n",
       "      (downsample): PatchMerging(\n",
       "        (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
       "        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (blocks): ModuleList(\n",
       "        (0): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=12, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.073)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.073)\n",
       "        )\n",
       "        (1): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=12, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.091)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.091)\n",
       "        )\n",
       "        (2): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=12, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.109)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.109)\n",
       "        )\n",
       "        (3): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=12, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.127)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.127)\n",
       "        )\n",
       "        (4): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=12, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.145)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.145)\n",
       "        )\n",
       "        (5): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=12, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.164)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.164)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): SwinTransformerV2Stage(\n",
       "      (downsample): PatchMerging(\n",
       "        (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (blocks): ModuleList(\n",
       "        (0): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=24, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.182)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.182)\n",
       "        )\n",
       "        (1): SwinTransformerV2Block(\n",
       "          (attn): WindowAttention(\n",
       "            (cpb_mlp): Sequential(\n",
       "              (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Linear(in_features=512, out_features=24, bias=False)\n",
       "            )\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            (softmax): Softmax(dim=-1)\n",
       "          )\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path1): DropPath(drop_prob=0.200)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (drop1): Dropout(p=0.0, inplace=False)\n",
       "            (norm): Identity()\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop2): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop_path2): DropPath(drop_prob=0.200)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (head): ClassifierHead(\n",
       "    (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Identity())\n",
       "    (drop): Dropout(p=0.5, inplace=False)\n",
       "    (fc): Linear(in_features=768, out_features=8, bias=True)\n",
       "    (flatten): Identity()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = timm.create_model(\n",
    "    'swinv2_tiny_window8_256.ms_in1k',\n",
    "    pretrained=True,\n",
    "    features_only=False,\n",
    "    num_classes=8,\n",
    "    # TODO: find out what these mean\n",
    "    drop_path_rate=0.2,\n",
    "    drop_rate=0.5\n",
    ")\n",
    "\n",
    "print(model.get_classifier())\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221\n",
      "Parameter 'patch_embed.proj.weight' requires grad.\n",
      "Parameter 'patch_embed.proj.bias' requires grad.\n",
      "Parameter 'patch_embed.norm.weight' requires grad.\n",
      "Parameter 'patch_embed.norm.bias' requires grad.\n",
      "Parameter 'layers.0.blocks.0.attn.logit_scale' requires grad.\n",
      "Parameter 'layers.0.blocks.0.attn.q_bias' requires grad.\n",
      "Parameter 'layers.0.blocks.0.attn.v_bias' requires grad.\n",
      "Parameter 'layers.0.blocks.0.attn.cpb_mlp.0.weight' requires grad.\n",
      "Parameter 'layers.0.blocks.0.attn.cpb_mlp.0.bias' requires grad.\n",
      "Parameter 'layers.0.blocks.0.attn.cpb_mlp.2.weight' requires grad.\n",
      "Parameter 'layers.0.blocks.0.attn.qkv.weight' requires grad.\n",
      "Parameter 'layers.0.blocks.0.attn.proj.weight' requires grad.\n",
      "Parameter 'layers.0.blocks.0.attn.proj.bias' requires grad.\n",
      "Parameter 'layers.0.blocks.0.norm1.weight' requires grad.\n",
      "Parameter 'layers.0.blocks.0.norm1.bias' requires grad.\n",
      "Parameter 'layers.0.blocks.0.mlp.fc1.weight' requires grad.\n",
      "Parameter 'layers.0.blocks.0.mlp.fc1.bias' requires grad.\n",
      "Parameter 'layers.0.blocks.0.mlp.fc2.weight' requires grad.\n",
      "Parameter 'layers.0.blocks.0.mlp.fc2.bias' requires grad.\n",
      "Parameter 'layers.0.blocks.0.norm2.weight' requires grad.\n",
      "Parameter 'layers.0.blocks.0.norm2.bias' requires grad.\n",
      "Parameter 'layers.0.blocks.1.attn.logit_scale' requires grad.\n",
      "Parameter 'layers.0.blocks.1.attn.q_bias' requires grad.\n",
      "Parameter 'layers.0.blocks.1.attn.v_bias' requires grad.\n",
      "Parameter 'layers.0.blocks.1.attn.cpb_mlp.0.weight' requires grad.\n",
      "Parameter 'layers.0.blocks.1.attn.cpb_mlp.0.bias' requires grad.\n",
      "Parameter 'layers.0.blocks.1.attn.cpb_mlp.2.weight' requires grad.\n",
      "Parameter 'layers.0.blocks.1.attn.qkv.weight' requires grad.\n",
      "Parameter 'layers.0.blocks.1.attn.proj.weight' requires grad.\n",
      "Parameter 'layers.0.blocks.1.attn.proj.bias' requires grad.\n",
      "Parameter 'layers.0.blocks.1.norm1.weight' requires grad.\n",
      "Parameter 'layers.0.blocks.1.norm1.bias' requires grad.\n",
      "Parameter 'layers.0.blocks.1.mlp.fc1.weight' requires grad.\n",
      "Parameter 'layers.0.blocks.1.mlp.fc1.bias' requires grad.\n",
      "Parameter 'layers.0.blocks.1.mlp.fc2.weight' requires grad.\n",
      "Parameter 'layers.0.blocks.1.mlp.fc2.bias' requires grad.\n",
      "Parameter 'layers.0.blocks.1.norm2.weight' requires grad.\n",
      "Parameter 'layers.0.blocks.1.norm2.bias' requires grad.\n",
      "Parameter 'layers.1.downsample.reduction.weight' requires grad.\n",
      "Parameter 'layers.1.downsample.norm.weight' requires grad.\n",
      "Parameter 'layers.1.downsample.norm.bias' requires grad.\n",
      "Parameter 'layers.1.blocks.0.attn.logit_scale' requires grad.\n",
      "Parameter 'layers.1.blocks.0.attn.q_bias' requires grad.\n",
      "Parameter 'layers.1.blocks.0.attn.v_bias' requires grad.\n",
      "Parameter 'layers.1.blocks.0.attn.cpb_mlp.0.weight' requires grad.\n",
      "Parameter 'layers.1.blocks.0.attn.cpb_mlp.0.bias' requires grad.\n",
      "Parameter 'layers.1.blocks.0.attn.cpb_mlp.2.weight' requires grad.\n",
      "Parameter 'layers.1.blocks.0.attn.qkv.weight' requires grad.\n",
      "Parameter 'layers.1.blocks.0.attn.proj.weight' requires grad.\n",
      "Parameter 'layers.1.blocks.0.attn.proj.bias' requires grad.\n",
      "Parameter 'layers.1.blocks.0.norm1.weight' requires grad.\n",
      "Parameter 'layers.1.blocks.0.norm1.bias' requires grad.\n",
      "Parameter 'layers.1.blocks.0.mlp.fc1.weight' requires grad.\n",
      "Parameter 'layers.1.blocks.0.mlp.fc1.bias' requires grad.\n",
      "Parameter 'layers.1.blocks.0.mlp.fc2.weight' requires grad.\n",
      "Parameter 'layers.1.blocks.0.mlp.fc2.bias' requires grad.\n",
      "Parameter 'layers.1.blocks.0.norm2.weight' requires grad.\n",
      "Parameter 'layers.1.blocks.0.norm2.bias' requires grad.\n",
      "Parameter 'layers.1.blocks.1.attn.logit_scale' requires grad.\n",
      "Parameter 'layers.1.blocks.1.attn.q_bias' requires grad.\n",
      "Parameter 'layers.1.blocks.1.attn.v_bias' requires grad.\n",
      "Parameter 'layers.1.blocks.1.attn.cpb_mlp.0.weight' requires grad.\n",
      "Parameter 'layers.1.blocks.1.attn.cpb_mlp.0.bias' requires grad.\n",
      "Parameter 'layers.1.blocks.1.attn.cpb_mlp.2.weight' requires grad.\n",
      "Parameter 'layers.1.blocks.1.attn.qkv.weight' requires grad.\n",
      "Parameter 'layers.1.blocks.1.attn.proj.weight' requires grad.\n",
      "Parameter 'layers.1.blocks.1.attn.proj.bias' requires grad.\n",
      "Parameter 'layers.1.blocks.1.norm1.weight' requires grad.\n",
      "Parameter 'layers.1.blocks.1.norm1.bias' requires grad.\n",
      "Parameter 'layers.1.blocks.1.mlp.fc1.weight' requires grad.\n",
      "Parameter 'layers.1.blocks.1.mlp.fc1.bias' requires grad.\n",
      "Parameter 'layers.1.blocks.1.mlp.fc2.weight' requires grad.\n",
      "Parameter 'layers.1.blocks.1.mlp.fc2.bias' requires grad.\n",
      "Parameter 'layers.1.blocks.1.norm2.weight' requires grad.\n",
      "Parameter 'layers.1.blocks.1.norm2.bias' requires grad.\n",
      "Parameter 'layers.2.downsample.reduction.weight' requires grad.\n",
      "Parameter 'layers.2.downsample.norm.weight' requires grad.\n",
      "Parameter 'layers.2.downsample.norm.bias' requires grad.\n",
      "Parameter 'layers.2.blocks.0.attn.logit_scale' requires grad.\n",
      "Parameter 'layers.2.blocks.0.attn.q_bias' requires grad.\n",
      "Parameter 'layers.2.blocks.0.attn.v_bias' requires grad.\n",
      "Parameter 'layers.2.blocks.0.attn.cpb_mlp.0.weight' requires grad.\n",
      "Parameter 'layers.2.blocks.0.attn.cpb_mlp.0.bias' requires grad.\n",
      "Parameter 'layers.2.blocks.0.attn.cpb_mlp.2.weight' requires grad.\n",
      "Parameter 'layers.2.blocks.0.attn.qkv.weight' requires grad.\n",
      "Parameter 'layers.2.blocks.0.attn.proj.weight' requires grad.\n",
      "Parameter 'layers.2.blocks.0.attn.proj.bias' requires grad.\n",
      "Parameter 'layers.2.blocks.0.norm1.weight' requires grad.\n",
      "Parameter 'layers.2.blocks.0.norm1.bias' requires grad.\n",
      "Parameter 'layers.2.blocks.0.mlp.fc1.weight' requires grad.\n",
      "Parameter 'layers.2.blocks.0.mlp.fc1.bias' requires grad.\n",
      "Parameter 'layers.2.blocks.0.mlp.fc2.weight' requires grad.\n",
      "Parameter 'layers.2.blocks.0.mlp.fc2.bias' requires grad.\n",
      "Parameter 'layers.2.blocks.0.norm2.weight' requires grad.\n",
      "Parameter 'layers.2.blocks.0.norm2.bias' requires grad.\n",
      "Parameter 'layers.2.blocks.1.attn.logit_scale' requires grad.\n",
      "Parameter 'layers.2.blocks.1.attn.q_bias' requires grad.\n",
      "Parameter 'layers.2.blocks.1.attn.v_bias' requires grad.\n",
      "Parameter 'layers.2.blocks.1.attn.cpb_mlp.0.weight' requires grad.\n",
      "Parameter 'layers.2.blocks.1.attn.cpb_mlp.0.bias' requires grad.\n",
      "Parameter 'layers.2.blocks.1.attn.cpb_mlp.2.weight' requires grad.\n",
      "Parameter 'layers.2.blocks.1.attn.qkv.weight' requires grad.\n",
      "Parameter 'layers.2.blocks.1.attn.proj.weight' requires grad.\n",
      "Parameter 'layers.2.blocks.1.attn.proj.bias' requires grad.\n",
      "Parameter 'layers.2.blocks.1.norm1.weight' requires grad.\n",
      "Parameter 'layers.2.blocks.1.norm1.bias' requires grad.\n",
      "Parameter 'layers.2.blocks.1.mlp.fc1.weight' requires grad.\n",
      "Parameter 'layers.2.blocks.1.mlp.fc1.bias' requires grad.\n",
      "Parameter 'layers.2.blocks.1.mlp.fc2.weight' requires grad.\n",
      "Parameter 'layers.2.blocks.1.mlp.fc2.bias' requires grad.\n",
      "Parameter 'layers.2.blocks.1.norm2.weight' requires grad.\n",
      "Parameter 'layers.2.blocks.1.norm2.bias' requires grad.\n",
      "Parameter 'layers.2.blocks.2.attn.logit_scale' requires grad.\n",
      "Parameter 'layers.2.blocks.2.attn.q_bias' requires grad.\n",
      "Parameter 'layers.2.blocks.2.attn.v_bias' requires grad.\n",
      "Parameter 'layers.2.blocks.2.attn.cpb_mlp.0.weight' requires grad.\n",
      "Parameter 'layers.2.blocks.2.attn.cpb_mlp.0.bias' requires grad.\n",
      "Parameter 'layers.2.blocks.2.attn.cpb_mlp.2.weight' requires grad.\n",
      "Parameter 'layers.2.blocks.2.attn.qkv.weight' requires grad.\n",
      "Parameter 'layers.2.blocks.2.attn.proj.weight' requires grad.\n",
      "Parameter 'layers.2.blocks.2.attn.proj.bias' requires grad.\n",
      "Parameter 'layers.2.blocks.2.norm1.weight' requires grad.\n",
      "Parameter 'layers.2.blocks.2.norm1.bias' requires grad.\n",
      "Parameter 'layers.2.blocks.2.mlp.fc1.weight' requires grad.\n",
      "Parameter 'layers.2.blocks.2.mlp.fc1.bias' requires grad.\n",
      "Parameter 'layers.2.blocks.2.mlp.fc2.weight' requires grad.\n",
      "Parameter 'layers.2.blocks.2.mlp.fc2.bias' requires grad.\n",
      "Parameter 'layers.2.blocks.2.norm2.weight' requires grad.\n",
      "Parameter 'layers.2.blocks.2.norm2.bias' requires grad.\n",
      "Parameter 'layers.2.blocks.3.attn.logit_scale' requires grad.\n",
      "Parameter 'layers.2.blocks.3.attn.q_bias' requires grad.\n",
      "Parameter 'layers.2.blocks.3.attn.v_bias' requires grad.\n",
      "Parameter 'layers.2.blocks.3.attn.cpb_mlp.0.weight' requires grad.\n",
      "Parameter 'layers.2.blocks.3.attn.cpb_mlp.0.bias' requires grad.\n",
      "Parameter 'layers.2.blocks.3.attn.cpb_mlp.2.weight' requires grad.\n",
      "Parameter 'layers.2.blocks.3.attn.qkv.weight' requires grad.\n",
      "Parameter 'layers.2.blocks.3.attn.proj.weight' requires grad.\n",
      "Parameter 'layers.2.blocks.3.attn.proj.bias' requires grad.\n",
      "Parameter 'layers.2.blocks.3.norm1.weight' requires grad.\n",
      "Parameter 'layers.2.blocks.3.norm1.bias' requires grad.\n",
      "Parameter 'layers.2.blocks.3.mlp.fc1.weight' requires grad.\n",
      "Parameter 'layers.2.blocks.3.mlp.fc1.bias' requires grad.\n",
      "Parameter 'layers.2.blocks.3.mlp.fc2.weight' requires grad.\n",
      "Parameter 'layers.2.blocks.3.mlp.fc2.bias' requires grad.\n",
      "Parameter 'layers.2.blocks.3.norm2.weight' requires grad.\n",
      "Parameter 'layers.2.blocks.3.norm2.bias' requires grad.\n",
      "Parameter 'layers.2.blocks.4.attn.logit_scale' requires grad.\n",
      "Parameter 'layers.2.blocks.4.attn.q_bias' requires grad.\n",
      "Parameter 'layers.2.blocks.4.attn.v_bias' requires grad.\n",
      "Parameter 'layers.2.blocks.4.attn.cpb_mlp.0.weight' requires grad.\n",
      "Parameter 'layers.2.blocks.4.attn.cpb_mlp.0.bias' requires grad.\n",
      "Parameter 'layers.2.blocks.4.attn.cpb_mlp.2.weight' requires grad.\n",
      "Parameter 'layers.2.blocks.4.attn.qkv.weight' requires grad.\n",
      "Parameter 'layers.2.blocks.4.attn.proj.weight' requires grad.\n",
      "Parameter 'layers.2.blocks.4.attn.proj.bias' requires grad.\n",
      "Parameter 'layers.2.blocks.4.norm1.weight' requires grad.\n",
      "Parameter 'layers.2.blocks.4.norm1.bias' requires grad.\n",
      "Parameter 'layers.2.blocks.4.mlp.fc1.weight' requires grad.\n",
      "Parameter 'layers.2.blocks.4.mlp.fc1.bias' requires grad.\n",
      "Parameter 'layers.2.blocks.4.mlp.fc2.weight' requires grad.\n",
      "Parameter 'layers.2.blocks.4.mlp.fc2.bias' requires grad.\n",
      "Parameter 'layers.2.blocks.4.norm2.weight' requires grad.\n",
      "Parameter 'layers.2.blocks.4.norm2.bias' requires grad.\n",
      "Parameter 'layers.2.blocks.5.attn.logit_scale' requires grad.\n",
      "Parameter 'layers.2.blocks.5.attn.q_bias' requires grad.\n",
      "Parameter 'layers.2.blocks.5.attn.v_bias' requires grad.\n",
      "Parameter 'layers.2.blocks.5.attn.cpb_mlp.0.weight' requires grad.\n",
      "Parameter 'layers.2.blocks.5.attn.cpb_mlp.0.bias' requires grad.\n",
      "Parameter 'layers.2.blocks.5.attn.cpb_mlp.2.weight' requires grad.\n",
      "Parameter 'layers.2.blocks.5.attn.qkv.weight' requires grad.\n",
      "Parameter 'layers.2.blocks.5.attn.proj.weight' requires grad.\n",
      "Parameter 'layers.2.blocks.5.attn.proj.bias' requires grad.\n",
      "Parameter 'layers.2.blocks.5.norm1.weight' requires grad.\n",
      "Parameter 'layers.2.blocks.5.norm1.bias' requires grad.\n",
      "Parameter 'layers.2.blocks.5.mlp.fc1.weight' requires grad.\n",
      "Parameter 'layers.2.blocks.5.mlp.fc1.bias' requires grad.\n",
      "Parameter 'layers.2.blocks.5.mlp.fc2.weight' requires grad.\n",
      "Parameter 'layers.2.blocks.5.mlp.fc2.bias' requires grad.\n",
      "Parameter 'layers.2.blocks.5.norm2.weight' requires grad.\n",
      "Parameter 'layers.2.blocks.5.norm2.bias' requires grad.\n",
      "Parameter 'layers.3.downsample.reduction.weight' requires grad.\n",
      "Parameter 'layers.3.downsample.norm.weight' requires grad.\n",
      "Parameter 'layers.3.downsample.norm.bias' requires grad.\n",
      "Parameter 'layers.3.blocks.0.attn.logit_scale' requires grad.\n",
      "Parameter 'layers.3.blocks.0.attn.q_bias' requires grad.\n",
      "Parameter 'layers.3.blocks.0.attn.v_bias' requires grad.\n",
      "Parameter 'layers.3.blocks.0.attn.cpb_mlp.0.weight' requires grad.\n",
      "Parameter 'layers.3.blocks.0.attn.cpb_mlp.0.bias' requires grad.\n",
      "Parameter 'layers.3.blocks.0.attn.cpb_mlp.2.weight' requires grad.\n",
      "Parameter 'layers.3.blocks.0.attn.qkv.weight' requires grad.\n",
      "Parameter 'layers.3.blocks.0.attn.proj.weight' requires grad.\n",
      "Parameter 'layers.3.blocks.0.attn.proj.bias' requires grad.\n",
      "Parameter 'layers.3.blocks.0.norm1.weight' requires grad.\n",
      "Parameter 'layers.3.blocks.0.norm1.bias' requires grad.\n",
      "Parameter 'layers.3.blocks.0.mlp.fc1.weight' requires grad.\n",
      "Parameter 'layers.3.blocks.0.mlp.fc1.bias' requires grad.\n",
      "Parameter 'layers.3.blocks.0.mlp.fc2.weight' requires grad.\n",
      "Parameter 'layers.3.blocks.0.mlp.fc2.bias' requires grad.\n",
      "Parameter 'layers.3.blocks.0.norm2.weight' requires grad.\n",
      "Parameter 'layers.3.blocks.0.norm2.bias' requires grad.\n",
      "Parameter 'layers.3.blocks.1.attn.logit_scale' requires grad.\n",
      "Parameter 'layers.3.blocks.1.attn.q_bias' requires grad.\n",
      "Parameter 'layers.3.blocks.1.attn.v_bias' requires grad.\n",
      "Parameter 'layers.3.blocks.1.attn.cpb_mlp.0.weight' requires grad.\n",
      "Parameter 'layers.3.blocks.1.attn.cpb_mlp.0.bias' requires grad.\n",
      "Parameter 'layers.3.blocks.1.attn.cpb_mlp.2.weight' requires grad.\n",
      "Parameter 'layers.3.blocks.1.attn.qkv.weight' requires grad.\n",
      "Parameter 'layers.3.blocks.1.attn.proj.weight' requires grad.\n",
      "Parameter 'layers.3.blocks.1.attn.proj.bias' requires grad.\n",
      "Parameter 'layers.3.blocks.1.norm1.weight' requires grad.\n",
      "Parameter 'layers.3.blocks.1.norm1.bias' requires grad.\n",
      "Parameter 'layers.3.blocks.1.mlp.fc1.weight' requires grad.\n",
      "Parameter 'layers.3.blocks.1.mlp.fc1.bias' requires grad.\n",
      "Parameter 'layers.3.blocks.1.mlp.fc2.weight' requires grad.\n",
      "Parameter 'layers.3.blocks.1.mlp.fc2.bias' requires grad.\n",
      "Parameter 'layers.3.blocks.1.norm2.weight' requires grad.\n",
      "Parameter 'layers.3.blocks.1.norm2.bias' requires grad.\n",
      "Parameter 'norm.weight' requires grad.\n",
      "Parameter 'norm.bias' requires grad.\n",
      "Parameter 'head.fc.weight' requires grad.\n",
      "Parameter 'head.fc.bias' requires grad.\n"
     ]
    }
   ],
   "source": [
    "print(len([param for param in model.named_parameters()]))\n",
    "\n",
    "# Iterate over the parameters and check requires_grad\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Parameter '{name}' requires grad.\")\n",
    "    else:\n",
    "        print(f\"Parameter '{name}' does not require grad.\")\n",
    "    \n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0, class_weights=None):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        probs = torch.sigmoid(logits)\n",
    "        ce_loss = nn.BCELoss()(probs, labels)\n",
    "        weight = (1 - probs).pow(self.gamma)\n",
    "        loss = ce_loss  # Initialize loss with cross-entropy loss\n",
    "        if self.class_weights is not None:\n",
    "            weight = weight * self.class_weights\n",
    "            loss = loss * weight\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "class_weights = class_weights.to(device)\n",
    "\n",
    "criterion = FocalLoss(class_weights)\n",
    "\n",
    "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.5)\n",
    "\n",
    "best_model_wts = model.state_dict()\n",
    "best_optimizer_state =optimizer.state_dict()\n",
    "best_acc = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "This section performs the actual training of the model. We first determine methods fit and validate that will be called during the training. Afterwards, we define the loop that optimizes the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, dataloader, optimizer,scheduler, criterion):\n",
    "    #print('Training')\n",
    "    model.train()\n",
    "    train_running_loss = 0.0\n",
    "    train_running_correct = 0\n",
    "    accum_iter = 4\n",
    "\n",
    "    for i, (inputs, labels) in enumerate(Bar(dataloader)):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.float().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        #model.zero_grad(set_to_none=True)\n",
    "        # Forward pass - compute outputs on input data using the model\n",
    "        outputs = model(inputs)\n",
    "        thresholds = [0.5, 0.5, 0.5,0.5,0.5,0.5,0.5,0.5]\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        train_running_loss += loss.item()* inputs.size(0)\n",
    "        # _ , preds = torch.max(outputs.data, 1)\n",
    "        # Apply sigmoid activation to obtain probabilities\n",
    "        #preds = (outputs > 0.5).float()\n",
    "        probs = torch.sigmoid(outputs)\n",
    "        preds = torch.zeros_like(probs)\n",
    "\n",
    "        # Set predicted labels based on the threshold\n",
    "        for i, threshold in enumerate(thresholds):\n",
    "            preds[:, i] = (probs[:, i] >= threshold).float()\n",
    "        train_running_correct += (preds == labels).all(dim=1).float().sum()\n",
    "        # Backpropagate the gradients\n",
    "        loss /= accum_iter\n",
    "        loss.backward()\n",
    "\n",
    "        if ((i + 1) % accum_iter == 0) :\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    train_loss = train_running_loss/len(dataloader.dataset)\n",
    "    train_accuracy = 100. * train_running_correct/len(dataloader.dataset)\n",
    "    return train_loss, train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader, optimizer, criterion):\n",
    "    #print('Validating')\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    val_running_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.float()\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            thresholds = [0.5, 0.5, 0.5,0.5,0.5,0.5,0.5,0.5]\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_running_loss += loss.item()*inputs.size(0)\n",
    "            #_, preds = torch.max(outputs.data, 1)\n",
    "            #preds = (outputs > 0.5).float()\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            preds = torch.zeros_like(probs)\n",
    "            # Set predicted labels based on the threshold\n",
    "            for i, threshold in enumerate(thresholds):\n",
    "                preds[:, i] = (probs[:, i] >= threshold).float()\n",
    "            val_running_correct += (preds == labels).all(dim=1).float().sum()\n",
    "\n",
    "    val_loss = val_running_loss/len(dataloader.dataset)\n",
    "    val_accuracy = 100. * val_running_correct/len(dataloader.dataset)\n",
    "    return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-1/50 lr: 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5694/5694: [===============================>] - ETA 0.7sss\n",
      "Train Loss: 0.1558, Train Acc: 63.52,Val Loss: 0.0885, Val Acc: 79.28,time : 111.13\n",
      "Epoch-2/50 lr: 0.0001\n",
      "5694/5694: [===============================>] - ETA 0.7sss\n",
      "Train Loss: 0.1058, Train Acc: 75.15,Val Loss: 0.0851, Val Acc: 81.92,time : 112.78\n",
      "Epoch-3/50 lr: 0.0001\n",
      "5694/5694: [===============================>] - ETA 0.7sss\n",
      "Train Loss: 0.0790, Train Acc: 82.17,Val Loss: 0.0466, Val Acc: 90.40,time : 113.28\n",
      "Epoch-4/50 lr: 0.0001\n",
      "5694/5694: [===============================>] - ETA 0.6sss\n",
      "Train Loss: 0.0634, Train Acc: 86.49,Val Loss: 0.0504, Val Acc: 90.19,time : 113.59\n",
      "Epoch-5/50 lr: 0.0001\n",
      "5694/5694: [===============================>] - ETA 0.7sss\n",
      "Train Loss: 0.0503, Train Acc: 89.62,Val Loss: 0.0638, Val Acc: 87.55,time : 114.61\n",
      "Epoch-6/50 lr: 0.0001\n",
      "5694/5694: [===============================>] - ETA 0.7sss\n",
      "Train Loss: 0.0431, Train Acc: 91.22,Val Loss: 0.0592, Val Acc: 91.10,time : 113.94\n",
      "Epoch-7/50 lr: 0.0001\n",
      "5694/5694: [===============================>] - ETA 0.7sss\n",
      "Train Loss: 0.0441, Train Acc: 91.80,Val Loss: 0.0381, Val Acc: 92.98,time : 113.04\n",
      "Epoch-8/50 lr: 0.0001\n",
      "5694/5694: [===============================>] - ETA 0.7sss\n",
      "Train Loss: 0.0378, Train Acc: 92.64,Val Loss: 0.0686, Val Acc: 89.78,time : 114.86\n",
      "Epoch-9/50 lr: 0.0001\n",
      "5694/5694: [===============================>] - ETA 0.6sss\n",
      "Train Loss: 0.0333, Train Acc: 93.66,Val Loss: 0.0251, Val Acc: 95.13,time : 113.75\n",
      "Epoch-10/50 lr: 0.0001\n",
      "5694/5694: [===============================>] - ETA 0.7sss\n",
      "Train Loss: 0.0303, Train Acc: 94.17,Val Loss: 0.0368, Val Acc: 93.95,time : 112.99\n",
      "Epoch-11/50 lr: 0.0001\n",
      "5698/5698: [===============================>] - ETA 0.7sss\n",
      "Train Loss: 0.0337, Train Acc: 93.93,Val Loss: 0.0097, Val Acc: 98.19,time : 113.45\n",
      "Epoch-12/50 lr: 0.0001\n",
      "5698/5698: [===============================>] - ETA 0.6sss\n",
      "Train Loss: 0.0326, Train Acc: 94.12,Val Loss: 0.0083, Val Acc: 98.47,time : 113.04\n",
      "Epoch-13/50 lr: 0.0001\n",
      "5698/5698: [===============================>] - ETA 0.7sss\n",
      "Train Loss: 0.0335, Train Acc: 94.10,Val Loss: 0.0104, Val Acc: 98.74,time : 113.44\n",
      "Epoch-14/50 lr: 0.0001\n",
      "5698/5698: [===============================>] - ETA 0.6sss\n",
      "Train Loss: 0.0240, Train Acc: 95.40,Val Loss: 0.0240, Val Acc: 96.79,time : 113.92\n",
      "Epoch-15/50 lr: 0.0001\n",
      "5698/5698: [===============================>] - ETA 0.6sss\n",
      "Train Loss: 0.0246, Train Acc: 96.21,Val Loss: 0.0168, Val Acc: 97.98,time : 113.59\n",
      "Epoch-16/50 lr: 5e-05\n",
      "5698/5698: [===============================>] - ETA 0.6sss\n",
      "Train Loss: 0.0119, Train Acc: 97.81,Val Loss: 0.0039, Val Acc: 99.37,time : 113.48\n",
      "Epoch-17/50 lr: 5e-05\n",
      "5698/5698: [===============================>] - ETA 0.7sss\n",
      "Train Loss: 0.0085, Train Acc: 98.32,Val Loss: 0.0065, Val Acc: 98.68,time : 113.47\n",
      "Epoch-18/50 lr: 5e-05\n",
      "5698/5698: [===============================>] - ETA 0.6sss\n",
      "Train Loss: 0.0072, Train Acc: 98.77,Val Loss: 0.0050, Val Acc: 99.37,time : 113.30\n",
      "Epoch-19/50 lr: 5e-05\n",
      "5698/5698: [===============================>] - ETA 0.7sss\n",
      "Train Loss: 0.0126, Train Acc: 97.98,Val Loss: 0.0150, Val Acc: 97.56,time : 113.08\n",
      "Epoch-20/50 lr: 5e-05\n",
      "5698/5698: [===============================>] - ETA 0.7sss\n",
      "Train Loss: 0.0107, Train Acc: 98.12,Val Loss: 0.0076, Val Acc: 98.95,time : 112.88\n",
      "Epoch-21/50 lr: 5e-05\n",
      "5704/5704: [===============================>] - ETA 0.7sss\n",
      "Train Loss: 0.0085, Train Acc: 98.35,Val Loss: 0.0003, Val Acc: 100.00,time : 114.63\n",
      "Epoch-22/50 lr: 5e-05\n",
      "5704/5704: [===============================>] - ETA 0.7sss\n",
      "Train Loss: 0.0111, Train Acc: 98.23,Val Loss: 0.0018, Val Acc: 99.72,time : 113.49\n",
      "Epoch-23/50 lr: 5e-05\n",
      "5704/5704: [===============================>] - ETA 0.7sss\n",
      "Train Loss: 0.0081, Train Acc: 98.54,Val Loss: 0.0019, Val Acc: 99.72,time : 114.14\n",
      "Epoch-24/50 lr: 5e-05\n",
      "5704/5704: [===============================>] - ETA 0.6sss\n",
      "Train Loss: 0.0076, Train Acc: 98.69,Val Loss: 0.0009, Val Acc: 99.86,time : 114.08\n",
      "Epoch-25/50 lr: 5e-05\n",
      "5704/5704: [===============================>] - ETA 0.7sss\n",
      "Train Loss: 0.0074, Train Acc: 98.63,Val Loss: 0.0036, Val Acc: 99.72,time : 113.88\n",
      "Epoch-26/50 lr: 5e-05\n",
      "5704/5704: [===============================>] - ETA 0.6sss\n",
      "Train Loss: 0.0076, Train Acc: 98.76,Val Loss: 0.0021, Val Acc: 99.65,time : 113.14\n",
      "Epoch-27/50 lr: 5e-05\n",
      "5704/5704: [===============================>] - ETA 0.7sss\n",
      "Train Loss: 0.0072, Train Acc: 98.74,Val Loss: 0.0021, Val Acc: 99.65,time : 113.35\n",
      "Epoch-28/50 lr: 5e-05\n",
      "5704/5704: [===============================>] - ETA 0.7sss\n",
      "Train Loss: 0.0078, Train Acc: 98.69,Val Loss: 0.0008, Val Acc: 99.86,time : 113.39\n",
      "Epoch-29/50 lr: 5e-05\n",
      "5704/5704: [===============================>] - ETA 0.7sss\n",
      "Train Loss: 0.0075, Train Acc: 98.65,Val Loss: 0.0006, Val Acc: 99.86,time : 114.89\n",
      "Epoch-30/50 lr: 5e-05\n",
      "5704/5704: [===============================>] - ETA 0.7sss\n",
      "Train Loss: 0.0071, Train Acc: 98.91,Val Loss: 0.0029, Val Acc: 99.65,time : 113.43\n",
      "Epoch-31/50 lr: 2.5e-05\n",
      "5713/5713: [===============================>] - ETA 0.7sss\n",
      "Train Loss: 0.0045, Train Acc: 99.25,Val Loss: 0.0001, Val Acc: 100.00,time : 113.85\n",
      "Epoch-32/50 lr: 2.5e-05\n",
      "5713/5713: [===============================>] - ETA 0.7sss\n",
      "Train Loss: 0.0048, Train Acc: 99.28,Val Loss: 0.0002, Val Acc: 100.00,time : 114.11\n",
      "Epoch-33/50 lr: 2.5e-05\n",
      "5713/5713: [===============================>] - ETA 0.6sss\n",
      "Train Loss: 0.0024, Train Acc: 99.58,Val Loss: 0.0002, Val Acc: 99.93,time : 113.25\n",
      "Epoch-34/50 lr: 2.5e-05\n",
      "5713/5713: [===============================>] - ETA 0.7sss\n",
      "Train Loss: 0.0046, Train Acc: 99.18,Val Loss: 0.0001, Val Acc: 100.00,time : 114.23\n",
      "Epoch-35/50 lr: 2.5e-05\n",
      "5713/5713: [===============================>] - ETA 0.6sss\n",
      "Train Loss: 0.0028, Train Acc: 99.58,Val Loss: 0.0006, Val Acc: 99.93,time : 113.40\n",
      "Epoch-36/50 lr: 2.5e-05\n",
      "5713/5713: [===============================>] - ETA 0.7sss\n",
      "Train Loss: 0.0017, Train Acc: 99.65,Val Loss: 0.0001, Val Acc: 100.00,time : 113.55\n",
      "Epoch-37/50 lr: 2.5e-05\n",
      "5713/5713: [===============================>] - ETA 0.6sss\n",
      "Train Loss: 0.0019, Train Acc: 99.61,Val Loss: 0.0008, Val Acc: 99.93,time : 113.43\n",
      "Epoch-38/50 lr: 2.5e-05\n",
      "5713/5713: [===============================>] - ETA 0.6sss\n",
      "Train Loss: 0.0030, Train Acc: 99.47,Val Loss: 0.0008, Val Acc: 99.72,time : 114.28\n",
      "Epoch-39/50 lr: 2.5e-05\n",
      "5713/5713: [===============================>] - ETA 0.7sss\n",
      "Train Loss: 0.0012, Train Acc: 99.75,Val Loss: 0.0001, Val Acc: 100.00,time : 114.63\n",
      "Epoch-40/50 lr: 2.5e-05\n",
      "5713/5713: [===============================>] - ETA 0.6sss\n",
      "Train Loss: 0.0032, Train Acc: 99.49,Val Loss: 0.0002, Val Acc: 100.00,time : 114.25\n",
      "Epoch-41/50 lr: 2.5e-05\n",
      "5719/5719: [===============================>] - ETA 0.7sss\n",
      "Train Loss: 0.0037, Train Acc: 99.42,Val Loss: 0.0002, Val Acc: 100.00,time : 115.13\n",
      "Epoch-42/50 lr: 2.5e-05\n",
      "5719/5719: [===============================>] - ETA 0.7sss\n",
      "Train Loss: 0.0030, Train Acc: 99.37,Val Loss: 0.0001, Val Acc: 100.00,time : 114.17\n",
      "Epoch-43/50 lr: 2.5e-05\n",
      "5719/5719: [===============================>] - ETA 0.7sss\n",
      "Train Loss: 0.0011, Train Acc: 99.76,Val Loss: 0.0000, Val Acc: 100.00,time : 115.45\n",
      "Epoch-44/50 lr: 2.5e-05\n",
      "5719/5719: [===============================>] - ETA 0.7sss\n",
      "Train Loss: 0.0047, Train Acc: 99.21,Val Loss: 0.0001, Val Acc: 100.00,time : 114.68\n",
      "Epoch-45/50 lr: 2.5e-05\n",
      "5719/5719: [===============================>] - ETA 0.7sss\n",
      "Train Loss: 0.0030, Train Acc: 99.55,Val Loss: 0.0001, Val Acc: 100.00,time : 114.85\n",
      "Epoch-46/50 lr: 1.25e-05\n",
      "5719/5719: [===============================>] - ETA 0.7sss\n",
      "Train Loss: 0.0023, Train Acc: 99.69,Val Loss: 0.0003, Val Acc: 99.93,time : 114.34\n",
      "Epoch-47/50 lr: 1.25e-05\n",
      "5719/5719: [===============================>] - ETA 0.7sss\n",
      "Train Loss: 0.0023, Train Acc: 99.70,Val Loss: 0.0001, Val Acc: 100.00,time : 113.27\n",
      "Epoch-48/50 lr: 1.25e-05\n",
      "5719/5719: [===============================>] - ETA 0.7sss\n",
      "Train Loss: 0.0014, Train Acc: 99.79,Val Loss: 0.0000, Val Acc: 100.00,time : 113.88\n",
      "Epoch-49/50 lr: 1.25e-05\n",
      "5719/5719: [===============================>] - ETA 0.6sss\n",
      "Train Loss: 0.0015, Train Acc: 99.67,Val Loss: 0.0000, Val Acc: 100.00,time : 113.94\n",
      "Epoch-50/50 lr: 1.25e-05\n",
      "5719/5719: [===============================>] - ETA 0.7sss\n",
      "Train Loss: 0.0013, Train Acc: 99.81,Val Loss: 0.0002, Val Acc: 99.93,time : 113.50\n"
     ]
    }
   ],
   "source": [
    "import time as time\n",
    "history=[]\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "#best_optimizer_state = copy.deepcopy(optimizer.state_dict())\n",
    "best_acc = 0.0\n",
    "epochs=50\n",
    "\n",
    "train_dataloader = train_dataloaders[0]\n",
    "validate_dataloader = validate_dataloaders[0]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_start = time.time()\n",
    "    print('Epoch-{0}/{1} lr: {2}'.format(epoch+1,epochs ,optimizer.param_groups[0]['lr']))\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        train_dataloader = train_dataloaders[int(epoch / 10)]\n",
    "        valid_dataloader = validate_dataloaders[int(epoch / 10)]\n",
    "\n",
    "    # Why is this here???\n",
    "    if  epoch > 14:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = True\n",
    "    #print(f\"Epoch {epoch+1} of {epochs}\")\n",
    "    train_epoch_loss, train_epoch_accuracy = fit(model,train_dataloader,optimizer,scheduler,criterion)\n",
    "    val_epoch_loss, val_epoch_accuracy = validate(model,valid_dataloader,optimizer,criterion)\n",
    "\n",
    "    epoch_end = time.time()\n",
    "    history.append([epoch+1,train_epoch_loss, train_epoch_accuracy, val_epoch_loss, val_epoch_accuracy,(epoch_end-epoch_start)])\n",
    "    print(f\"Train Loss: {train_epoch_loss:.4f}, Train Acc: {train_epoch_accuracy:.2f},Val Loss: {val_epoch_loss:.4f}, Val Acc: {val_epoch_accuracy:.2f},time : {epoch_end-epoch_start:.2f}\")\n",
    "    torch.save({'history':history},'Master_his.pth')\n",
    "    if val_epoch_accuracy > best_acc:\n",
    "        best_acc = val_epoch_accuracy\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        best_epoch=epoch\n",
    "        torch.save({\n",
    "            'epoch': epoch+1,\n",
    "            'model_state_dict': best_model_wts,\n",
    "            'loss': criterion,\n",
    "            'history':history,\n",
    "            'best_epoch': best_epoch+1,\n",
    "\n",
    "            }, 'Master.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
